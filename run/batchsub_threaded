#!/bin/bash -l
#SBATCH --job-name="ESM4"
#SBATCH --account="s1201"
#SBATCH --mail-type=NONE
#SBATCH --mail-user=aaron.wienkers@unibe.ch
#SBATCH --time=00:10:00
#SBATCH --partition=normal
#SBATCH --constraint=gpu
#SBATCH --hint=nomultithread
#SBATCH --ntasks-per-core=1
#SBATCH --nodes=32 --ntasks-per-node=6  --cpus-per-task=2 --ntasks=192
#SBATCH hetjob
#SBATCH --nodes=10 --ntasks-per-node=12 --cpus-per-task=1 --ntasks=120

# cpus-per-task == MPI Threads
# ntasks == "atm_cores" and "ocn_cores"
# ntasks-per-node * cpus-per-task == 12 (or however many cores are on each node for the partition)
# nodes * ntasks-per-node == ntasks



# Setup Environment
module load daint-gpu
module switch PrgEnv-cray PrgEnv-intel
module unload cray-libsci
module load cray-netcdf-hdf5parallel
export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH

# Setup Execution Variables
workDir=/scratch/snx3000/awienker/ESM4_rundir/
executable=${PWD}/../exec/esm4.1.x
run_prog=srun

# Number of cores to run the atmosphere
atm_cores=192
# Number of threads to use for the atmosphere
atm_threads=2
# Number of cores to run the ocean
ocn_cores=120

###########################################################
##  ############# END USER INPUT SECTION ############### ##
###########################################################
## Set environment variables
export KMP_STACKSIZE=512m
export NC_BLKSZ=1M
export F_UFMTENDIAN=big

## Set the stacksize to unlimited
ulimit unlimited
ulimit -S -s unlimited
ulimit -S -c unlimited

## Go to the workDir
cd ${workDir}

## Execute the model in the workDir
${run_prog} -n ${atm_cores} -c ${atm_threads} ${executable} : -n ${ocn_cores} -c 1 ${executable} |& tee stdout.log

